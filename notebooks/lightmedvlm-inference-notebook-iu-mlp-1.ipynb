{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install lightning\n",
    "!pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:24:37.315352Z",
     "iopub.status.busy": "2025-11-22T13:24:37.314603Z",
     "iopub.status.idle": "2025-11-22T13:24:41.912719Z",
     "shell.execute_reply": "2025-11-22T13:24:41.911971Z",
     "shell.execute_reply.started": "2025-11-22T13:24:37.315328Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"huyhoangt2201/lightmedvlm-iu-mlp-20epochs-tune-vision-encoder\",\n",
    "    local_dir=\"lightmedvlm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:30:19.890008Z",
     "iopub.status.busy": "2025-11-22T13:30:19.889727Z",
     "iopub.status.idle": "2025-11-22T13:30:19.916880Z",
     "shell.execute_reply": "2025-11-22T13:30:19.916120Z",
     "shell.execute_reply.started": "2025-11-22T13:30:19.889988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import lightning.pytorch as pl\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import SwinModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import torch.distributed as dist\n",
    "from transformers import BertTokenizer, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, inter_dim, out_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.hidden_1 = nn.Linear(in_dim, inter_dim)\n",
    "        self.act = nn.GELU()\n",
    "        self.hidden_2 = nn.Linear(inter_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.act(self.hidden_1(x))\n",
    "        x = self.dropout(x)\n",
    "        return self.hidden_2(x)\n",
    "\n",
    "\n",
    "class LightMedVLM(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vision_model: str = \"microsoft/swin-base-patch4-window7-224\",\n",
    "        llm_model: str = \"Qwen/Qwen3-0.6B\",\n",
    "\n",
    "        # For training setup\n",
    "        vis_use_lora: bool = False,\n",
    "        vis_r: int = 8,\n",
    "        vis_alpha: int = 16,\n",
    "        freeze_vm: bool = False,\n",
    "        llm_use_lora: bool = False,\n",
    "        llm_r: int = 8,\n",
    "        llm_alpha: int = 16,\n",
    "        lora_dropout: float = 0.1,\n",
    "        low_resource: bool = False,\n",
    "        max_length: int = 256\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vision_model = vision_model\n",
    "        self.llm_model = llm_model\n",
    "        self.vis_use_lora = vis_use_lora\n",
    "        self.vis_r = vis_r\n",
    "        self.vis_alpha = vis_alpha\n",
    "        self.freeze_vm = freeze_vm\n",
    "        self.llm_use_lora = llm_use_lora\n",
    "        self.llm_r = llm_r\n",
    "        self.llm_alpha = llm_alpha\n",
    "        self.lora_dropout = lora_dropout\n",
    "        self.low_resource = low_resource\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Vision encoder setup\n",
    "        print(f'Loading vision encoder: {self.vision_model}')\n",
    "        self.visual_encoder = SwinModel.from_pretrained(self.vision_model)\n",
    "        self.vit_feature_extractor = AutoImageProcessor.from_pretrained(self.vision_model)\n",
    "        if self.vis_use_lora:\n",
    "            peft_config_visual = LoraConfig(\n",
    "                r=self.vis_r,\n",
    "                lora_alpha=self.vis_alpha,\n",
    "                target_modules=[\"query\", \"value\"],\n",
    "                lora_dropout=self.lora_dropout,\n",
    "                bias=\"none\",\n",
    "                modules_to_save=[\"classifier\"],\n",
    "            )\n",
    "            self.visual_encoder = get_peft_model(self.visual_encoder, peft_config_visual)\n",
    "            self.visual_encoder.print_trainable_parameters()\n",
    "            print('Loading vision encoder with LoRA -- Done')\n",
    "        elif self.freeze_vm:\n",
    "            for name, param in self.visual_encoder.named_parameters():\n",
    "                param.requires_grad = False\n",
    "            print(f'Loading Frozen vision encoder: {self.vision_model} -- Done')\n",
    "        else:\n",
    "            print(f'Loading Trainable vision encoder: {self.vision_model} -- Done')\n",
    "\n",
    "        # LLM model setup\n",
    "        print('Loading LLM model')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.llm_model, \n",
    "            use_fast=False,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        print(f\"BOS token ID: {self.tokenizer.bos_token_id}\")\n",
    "        print(f\"EOS token ID: {self.tokenizer.eos_token_id}\")\n",
    "        print(f\"PAD token ID: {self.tokenizer.pad_token_id}\")\n",
    "        if self.low_resource:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.llm_model,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                load_in_8bit=True,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        else:\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.llm_model,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "        if self.llm_use_lora:\n",
    "            self.embed_tokens = self.model.get_input_embeddings()\n",
    "            peft_config = LoraConfig(\n",
    "                task_type=TaskType.CAUSAL_LM, \n",
    "                inference_mode=False, \n",
    "                r=self.llm_r, \n",
    "                lora_alpha=self.llm_alpha, \n",
    "                lora_dropout=self.lora_dropout,\n",
    "                target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]  \n",
    "            )\n",
    "            self.model = get_peft_model(self.model, peft_config)\n",
    "            self.model.print_trainable_parameters()\n",
    "            print('Loading LLM LoRA Done')         \n",
    "        else:\n",
    "            self.embed_tokens = self.model.get_input_embeddings()\n",
    "            for name, param in self.model.named_parameters():\n",
    "                param.requires_grad = False\n",
    "            print('Loading LLM Done')\n",
    "\n",
    "        # Projector setup\n",
    "        self.proj = MLP(\n",
    "            in_dim=self.visual_encoder.num_features,\n",
    "            inter_dim=2048,\n",
    "            out_dim=self.model.config.hidden_size\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(self.model.config.hidden_size)\n",
    "\n",
    "        # System prompt setup\n",
    "        self.end_sym = \"<|im_end|>\"\n",
    "        self.prompt = \"<|im_start|>system\\nYou are a professional radiologist. Please generate a comprehensive and detailed diagnosis report for this chest xray image.<|im_end|>\\n<|im_start|>user\\nThe chest X-ray image shows:\"\n",
    "        \n",
    "    def encode_img(self, images):\n",
    "        image_embeds = []\n",
    "        for image in images:\n",
    "            device = image.device\n",
    "            # Swin transformer\n",
    "            visual_outputs = self.visual_encoder(image)\n",
    "            image_embed = visual_outputs['last_hidden_state'].to(device)\n",
    "            image_embeds.append(image_embed)\n",
    "            \n",
    "        image_embeds = torch.stack(image_embeds).mean(0)\n",
    "\n",
    "        inputs = self.proj(image_embeds)\n",
    "        atts = torch.ones(inputs.size()[:-1], dtype=torch.long).to(image.device)\n",
    "        return inputs, atts\n",
    "\n",
    "    def prompt_wrap(self, img_embeds, atts_img):\n",
    "        \"\"\"\n",
    "        Wrap image embeddings with Qwen-style prompt.\n",
    "        Format: {prompt_before} <image> {prompt_after}\n",
    "        \"\"\"\n",
    "        # Full prompt with image placeholder\n",
    "        full_prompt = f\"{self.prompt} <image><|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "        \n",
    "        batch_size = img_embeds.shape[0]\n",
    "        \n",
    "        # Split prompt at image placeholder\n",
    "        p_before, p_after = full_prompt.split('<image>')\n",
    "        \n",
    "        # Tokenize prompt parts (no special tokens added)\n",
    "        p_before_tokens = self.tokenizer(\n",
    "            p_before,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False\n",
    "        ).to(img_embeds.device)\n",
    "        \n",
    "        p_after_tokens = self.tokenizer(\n",
    "            p_after,\n",
    "            return_tensors=\"pt\",\n",
    "            add_special_tokens=False\n",
    "        ).to(img_embeds.device)\n",
    "        \n",
    "        # Get embeddings using frozen embed_tokens\n",
    "        with torch.no_grad():\n",
    "            p_before_embeds = self.embed_tokens(p_before_tokens.input_ids).expand(batch_size, -1, -1)\n",
    "            p_after_embeds = self.embed_tokens(p_after_tokens.input_ids).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Concatenate: [prompt_before] + [image] + [prompt_after]\n",
    "        wrapped_img_embeds = torch.cat([p_before_embeds, img_embeds, p_after_embeds], dim=1)\n",
    "        \n",
    "        # Create attention mask for entire sequence\n",
    "        wrapped_atts_img = torch.ones(\n",
    "            batch_size, \n",
    "            wrapped_img_embeds.shape[1], \n",
    "            device=img_embeds.device, \n",
    "            dtype=atts_img.dtype\n",
    "        )\n",
    "        \n",
    "        return wrapped_img_embeds, wrapped_atts_img\n",
    "\n",
    "    def forward(self, samples):\n",
    "        image = samples[\"image\"]\n",
    "        img_embeds, atts_img = self.encode_img(image)\n",
    "        img_embeds = self.layer_norm(img_embeds)\n",
    "\n",
    "        # Wrap image with prompt\n",
    "        img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img)\n",
    "\n",
    "        self.tokenizer.padding_side = \"right\"\n",
    "        text = [t + self.end_sym for t in samples[\"text\"]]\n",
    "\n",
    "        # Tokenize target text\n",
    "        to_regress_tokens = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            add_special_tokens=False\n",
    "        ).to(image[0].device)\n",
    "\n",
    "        # Create labels: mask prompt+image tokens with -100, keep text tokens\n",
    "        targets = to_regress_tokens.input_ids.masked_fill(\n",
    "            to_regress_tokens.input_ids == self.tokenizer.pad_token_id, -100\n",
    "        )\n",
    "\n",
    "        # Create empty targets for prompt+image tokens\n",
    "        empty_targets = (\n",
    "            torch.ones([atts_img.shape[0], atts_img.shape[1]],\n",
    "                       dtype=torch.long).to(image[0].device).fill_(-100)\n",
    "        )\n",
    "        targets = torch.cat([empty_targets, targets], dim=1)\n",
    "\n",
    "        # Get text embeddings\n",
    "        with torch.no_grad():\n",
    "            to_regress_embeds = self.embed_tokens(to_regress_tokens.input_ids)\n",
    "        \n",
    "        # Concatenate all embeddings: [prompt+image] + [text]\n",
    "        inputs_embeds = torch.cat([img_embeds, to_regress_embeds], dim=1)\n",
    "        attention_mask = torch.cat([atts_img, to_regress_tokens.attention_mask], dim=1)\n",
    "\n",
    "        # Forward through LLM\n",
    "        outputs = self.model(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True,\n",
    "            labels=targets,\n",
    "            use_cache=False\n",
    "        )\n",
    "        all_loss = outputs.loss\n",
    "\n",
    "        return {\"loss\": all_loss}\n",
    "\n",
    "    def decode(self, output_token):\n",
    "        \"\"\"Decode output tokens to text.\"\"\"\n",
    "        # Remove special tokens at the beginning\n",
    "        if len(output_token) > 0 and output_token[0] == self.tokenizer.pad_token_id:\n",
    "            output_token = output_token[1:]\n",
    "        if len(output_token) > 0 and output_token[0] == self.tokenizer.bos_token_id:\n",
    "            output_token = output_token[1:]\n",
    "        \n",
    "        # Decode to text\n",
    "        output_text = self.tokenizer.decode(output_token, add_special_tokens=False)\n",
    "        \n",
    "        # Split at end symbol and clean up\n",
    "        output_text = output_text.split(self.end_sym)[0].strip()\n",
    "        \n",
    "        # Remove Qwen special tokens\n",
    "        output_text = output_text.replace('<|im_start|>', '')\n",
    "        output_text = output_text.replace('<|im_end|>', '')\n",
    "        output_text = output_text.replace('<|endoftext|>', '')\n",
    "        output_text = output_text.replace('<unk>', '')\n",
    "        \n",
    "        return output_text\n",
    "\n",
    "    def _parse_image(self, img):\n",
    "        pixel_values = self.vit_feature_extractor(img, return_tensors=\"pt\").pixel_values\n",
    "        return pixel_values[0] \n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def inference(self, image_paths, \n",
    "                        beam_size=3, \n",
    "                        do_sample=False, \n",
    "                        min_new_tokens=10, \n",
    "                        max_new_tokens=120, \n",
    "                        repetition_penalty=2.0, \n",
    "                        length_penalty=2.0, \n",
    "                        temperature=0):\n",
    "        \"\"\"Generate text from images.\"\"\"\n",
    "        self.eval()\n",
    "\n",
    "        images = []\n",
    "        device = next(self.parameters()).device\n",
    "        for image_path in image_paths:\n",
    "            with Image.open(image_path) as pil:\n",
    "                array = np.array(pil, dtype=np.uint8)\n",
    "                if array.shape[-1] != 3 or len(array.shape) != 3:\n",
    "                    array = np.array(pil.convert(\"RGB\"), dtype=np.uint8)\n",
    "                image = self._parse_image(array)\n",
    "                image = image.unsqueeze(0).to(device)\n",
    "                images.append(image)\n",
    "\n",
    "\n",
    "        dtype = self.model.dtype\n",
    "        \n",
    "        img_embeds, atts_img = self.encode_img(images)\n",
    "        img_embeds = self.layer_norm(img_embeds)\n",
    "\n",
    "        img_embeds = img_embeds.to(dtype)\n",
    "        img_embeds, atts_img = self.prompt_wrap(img_embeds, atts_img)\n",
    "\n",
    "        outputs = self.model.generate(\n",
    "            inputs_embeds=img_embeds,\n",
    "            attention_mask=atts_img,\n",
    "            num_beams=beam_size,\n",
    "            do_sample=do_sample,\n",
    "            min_new_tokens=min_new_tokens,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            temperature=temperature,\n",
    "            pad_token_id=self.tokenizer.pad_token_id,\n",
    "            eos_token_id=self.tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        out = [self.decode(i) for i in outputs]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:30:24.663112Z",
     "iopub.status.busy": "2025-11-22T13:30:24.662838Z",
     "iopub.status.idle": "2025-11-22T13:30:26.458838Z",
     "shell.execute_reply": "2025-11-22T13:30:26.458001Z",
     "shell.execute_reply.started": "2025-11-22T13:30:24.663092Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ckpt_file=\"lightmedvlm/checkpoints/checkpoint_epoch15_step4128_bleu0.096789_cider0.022736_renamed.pth\"   # Absoluate path to .pth file\n",
    "args = {\n",
    "    \"vision_model\":\"microsoft/swin-base-patch4-window7-224\",\n",
    "    \"llm_model\":\"Qwen/Qwen3-0.6B\"\n",
    "}\n",
    "model = LightMedVLM.load_from_checkpoint(ckpt_file,strict=False, **args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-22T13:30:29.920313Z",
     "iopub.status.busy": "2025-11-22T13:30:29.919759Z",
     "iopub.status.idle": "2025-11-22T13:30:50.158917Z",
     "shell.execute_reply": "2025-11-22T13:30:50.158226Z",
     "shell.execute_reply.started": "2025-11-22T13:30:29.920288Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.inference([\"/kaggle/input/iu-xray/iu_xray/images/CXR100_IM-0002/0.png\", \"/kaggle/input/iu-xray/iu_xray/images/CXR100_IM-0002/1.png\"])  # Inference one report at a time"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7493627,
     "sourceId": 11919708,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
