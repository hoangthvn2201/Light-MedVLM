{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install unsloth\n",
    "!pip instal peft==0.5.0\n",
    "!pip install bitsandbytes\n",
    "!pip install trl\n",
    "!pip install dotenv\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T08:49:41.962493Z",
     "iopub.status.busy": "2025-09-28T08:49:41.962169Z",
     "iopub.status.idle": "2025-09-28T08:49:41.967050Z",
     "shell.execute_reply": "2025-09-28T08:49:41.966332Z",
     "shell.execute_reply.started": "2025-09-28T08:49:41.962473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"google/gemma-3-270m\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "DTYPE = None\n",
    "LOAD_IN_4BIT = False\n",
    "LOAD_IN_8BIT = False\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "USE_LORA = False\n",
    "OUTPUT_DIR = \"./sft_trainer\"\n",
    "RUN_NAME = f\"{str(MODEL_ID.split('/')[-1])}_{EPOCHS}Epochs_Lora_{USE_LORA}_BATCH_SIZE_{BATCH_SIZE}\"\n",
    "HF_TOKEN = None\n",
    "WANDB_API_KEY = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T08:49:46.508569Z",
     "iopub.status.busy": "2025-09-28T08:49:46.507868Z",
     "iopub.status.idle": "2025-09-28T08:49:56.178803Z",
     "shell.execute_reply": "2025-09-28T08:49:56.178168Z",
     "shell.execute_reply.started": "2025-09-28T08:49:46.508546Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "import torch \n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# HF_TOKEN = os.getenv('HF_TOKEN', None)\n",
    "# WANDB_API_KEY = os.getenv('WANDB_API_KEY', None)\n",
    "if WANDB_API_KEY is not None:\n",
    "    wandb.login(key=WANDB_API_KEY)\n",
    "    wandb.init(project=\"ft_proj\", name=RUN_NAME)\n",
    "\n",
    "if HF_TOKEN != None:\n",
    "    login(token=HF_TOKEN)\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_ID,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = DTYPE,\n",
    "    load_in_4bit = LOAD_IN_4BIT,\n",
    "    load_in_8bit = LOAD_IN_8BIT,\n",
    "    full_finetuning = False if USE_LORA else True,\n",
    ")\n",
    "\n",
    "\n",
    "if USE_LORA:\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=32,  \n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0,\n",
    "        bias=\"none\",    \n",
    "        use_gradient_checkpointing=\"unsloth\", \n",
    "        random_state=3407,\n",
    "        use_rslora=False,   \n",
    "        loftq_config=None, \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T08:17:17.783648Z",
     "iopub.status.busy": "2025-09-28T08:17:17.783108Z",
     "iopub.status.idle": "2025-09-28T08:17:19.222097Z",
     "shell.execute_reply": "2025-09-28T08:17:19.221327Z",
     "shell.execute_reply.started": "2025-09-28T08:17:17.783626Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"keivalya/MedQuad-MedicalQnADataset\", split='train')\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    questions = examples['Question']\n",
    "    answers = examples['Answer']\n",
    "    texts = []\n",
    "    for question, answer in zip(questions, answers):\n",
    "        if tokenizer.chat_template != None:\n",
    "            prompt = [\n",
    "                {\"role\": \"user\", \"content\": question},\n",
    "                {\"role\": \"assistant\", \"content\": answer}\n",
    "            ]\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                prompt, tokenize=False, add_generation_prompt=False\n",
    "            )\n",
    "            texts.append(text)\n",
    "        else: \n",
    "            gemma_template_prompt = \"<start_of_turn>user\\n{question}<end_of_turn><start_of_turn>model\\n{answer}<end_of_turn>\"\n",
    "            text = gemma_template_prompt.format(question=question, answer=answer)\n",
    "            texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, batch_size=8)\n",
    "dataset_split = dataset.train_test_split(test_size=0.05)\n",
    "train_ds = dataset_split['train']\n",
    "test_ds = dataset_split['test']\n",
    "print(f\"Train ds: {len(train_ds)}\")\n",
    "print(f\"Test ds: {len(test_ds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T08:39:20.222761Z",
     "iopub.status.busy": "2025-09-28T08:39:20.222480Z",
     "iopub.status.idle": "2025-09-28T08:39:20.226836Z",
     "shell.execute_reply": "2025-09-28T08:39:20.226315Z",
     "shell.execute_reply.started": "2025-09-28T08:39:20.222741Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "from transformers import get_scheduler, EarlyStoppingCallback, TrainerCallback\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T08:17:24.080643Z",
     "iopub.status.busy": "2025-09-28T08:17:24.080373Z",
     "iopub.status.idle": "2025-09-28T08:17:24.089820Z",
     "shell.execute_reply": "2025-09-28T08:17:24.089108Z",
     "shell.execute_reply.started": "2025-09-28T08:17:24.080625Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LossLoggingCallback(TrainerCallback):\n",
    "    \"\"\"Custom callback to save training and validation losses to a text file.\"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir):\n",
    "        self.output_dir = output_dir\n",
    "        self.loss_file = os.path.join(output_dir, \"training_losses.txt\")\n",
    "        self.losses = []\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        with open(self.loss_file, 'w') as f:\n",
    "            f.write(\"Epoch\\tStep\\tTraining_Loss\\tValidation_Loss\\tLearning_Rate\\n\")\n",
    "    \n",
    "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
    "        \"\"\"Called when logging occurs during training.\"\"\"\n",
    "        if logs is not None:\n",
    "            current_log = {\n",
    "                'epoch': state.epoch,\n",
    "                'step': state.global_step,\n",
    "                'train_loss': logs.get('train_loss', None),\n",
    "                'eval_loss': logs.get('eval_loss', None),\n",
    "                'learning_rate': logs.get('learning_rate', None)\n",
    "            }\n",
    "            self.losses.append(current_log)\n",
    "            with open(self.loss_file, 'a') as f:\n",
    "                f.write(f\"{current_log['epoch']:.2f}\\t\"\n",
    "                       f\"{current_log['step']}\\t\"\n",
    "                       f\"{current_log['train_loss'] if current_log['train_loss'] else 'N/A'}\\t\"\n",
    "                       f\"{current_log['eval_loss'] if current_log['eval_loss'] else 'N/A'}\\t\"\n",
    "                       f\"{current_log['learning_rate'] if current_log['learning_rate'] else 'N/A'}\\n\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, model=None, **kwargs):\n",
    "        \"\"\"Called at the end of training to save a summary.\"\"\"\n",
    "        summary_file = os.path.join(self.output_dir, \"training_summary.json\")\n",
    "        summary = {\n",
    "            'total_epochs': state.epoch,\n",
    "            'total_steps': state.global_step,\n",
    "            'final_train_loss': self.losses[-1]['train_loss'] if self.losses else None,\n",
    "            'final_eval_loss': self.losses[-1]['eval_loss'] if self.losses else None,\n",
    "            'best_eval_loss': min([log['eval_loss'] for log in self.losses if log['eval_loss'] is not None], default=None),\n",
    "            'all_losses': self.losses\n",
    "        }\n",
    "        with open(summary_file, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "        \n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Loss logs saved to: {self.loss_file}\")\n",
    "        print(f\"Training summary saved to: {summary_file}\")\n",
    "\n",
    "loss_callback = LossLoggingCallback(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = train_ds,\n",
    "    eval_dataset = test_ds,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    \n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = BATCH_SIZE,\n",
    "        gradient_accumulation_steps = BATCH_SIZE,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = EPOCHS,\n",
    "        learning_rate = LEARNING_RATE,\n",
    "        logging_steps = 5,\n",
    "\n",
    "        eval_strategy = \"epoch\",\n",
    "        save_strategy = \"epoch\",\n",
    "        save_total_limit = 1,\n",
    "        load_best_model_at_end = True, \n",
    "        metric_for_best_model = 'eval_loss',\n",
    "        greater_is_better = False,\n",
    "\n",
    "        # max_steps = 100, # Run by steps\n",
    "        # eval_strategy = \"steps\",  \n",
    "        # eval_steps = 100,  \n",
    "        # save_strategy = \"steps\",  \n",
    "        # save_steps = 100, \n",
    "        # save_total_limit = 1,\n",
    "        # load_best_model_at_end = True, \n",
    "        # metric_for_best_model = 'eval_loss',\n",
    "        # greater_is_better = False,\n",
    "        \n",
    "\n",
    "        optim = \"adamw_8bit\", \n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"cosine\",\n",
    "        seed = 3047,\n",
    "        logging_dir = os.path.join(OUTPUT_DIR, 'logs'),\n",
    "        report_to = \"none\", \n",
    "        # run_name = RUN_NAME\n",
    "    ),\n",
    "    callbacks = [\n",
    "        EarlyStoppingCallback(early_stopping_patience=5),\n",
    "        loss_callback]      \n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "trainer.model.push_to_hub(f\"huyhoangt2201/{RUN_NAME}\")\n",
    "tokenizer.push_to_hub(f\"huyhoangt2201/{RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T08:52:58.980350Z",
     "iopub.status.busy": "2025-09-28T08:52:58.980047Z",
     "iopub.status.idle": "2025-09-28T08:52:58.995338Z",
     "shell.execute_reply": "2025-09-28T08:52:58.994563Z",
     "shell.execute_reply.started": "2025-09-28T08:52:58.980332Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T08:53:31.543680Z",
     "iopub.status.busy": "2025-09-28T08:53:31.542973Z",
     "iopub.status.idle": "2025-09-28T08:53:58.764530Z",
     "shell.execute_reply": "2025-09-28T08:53:58.763843Z",
     "shell.execute_reply.started": "2025-09-28T08:53:31.543656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.model.push_to_hub(f\"huyhoangt2201/{RUN_NAME}\")\n",
    "tokenizer.push_to_hub(f\"huyhoangt2201/{RUN_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
